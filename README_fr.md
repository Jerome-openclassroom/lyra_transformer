# Lyra_Transformer_Training

ğŸ§  Fine-tuning et Ã©valuation dâ€™un modÃ¨le Transformer DistilBERT sur une tÃ¢che personnalisÃ©e de classification de nouvelles (tech, science, business, sports).  
Ce dÃ©pÃ´t fait partie de la sÃ©rie Lyra â€” projets IA ouverts, scientifiques et pÃ©dagogiques, centrÃ©s sur la synergie humainâ€“IA.

---

## ğŸŒ AperÃ§u

Ce projet dÃ©montre le fine-tuning dâ€™un Transformer lÃ©ger (DistilBERT) sur un petit jeu de donnÃ©es Ã©tiquetÃ© (format CSV) en utilisant les bibliothÃ¨ques `transformers` et `datasets` de Hugging Face.  
Il inclut :  
- Lâ€™entraÃ®nement Ã  la fois en **Jupyter Notebook** et **Google Colab**  
- Lâ€™encodage des labels et le prÃ©traitement avec tokenizer  
- La sauvegarde et le rechargement du modÃ¨le fine-tunÃ©  
- Lâ€™Ã©valuation sur des exemples de test personnalisÃ©s  
- Lâ€™intÃ©gration GitHub et une configuration reproductible  

---

## ğŸ§¾ Classes et Labels

La tÃ¢che de classification couvre 4 catÃ©gories :

| ID Label | Classe    |
|----------|-----------|
| 0        | business  |
| 1        | science   |
| 2        | sports    |
| 3        | tech      |

---

## ğŸ§ª ModÃ¨le

ğŸ‘‰ Le modÃ¨le entraÃ®nÃ© nâ€™est pas inclus dans ce dÃ©pÃ´t pour des raisons de taille.

- ğŸ§  Base : `distilbert-base-uncased`  
- Optimiseur : `AdamW` (gÃ©rÃ© par `Trainer`)  
- Learning rate : `2e-5`  
- Weight decay : `0.01`  
- Batch size : `16`  
- Longueur max. des sÃ©quences : `128`  
- Ã‰poques : `1` (Colab), `3` (Jupyter)  
- Tokenizer padding : `max_length` avec troncature  
- RÃ©gularisation : L2 sur les poids uniquement (biais exclus)  

---

## ğŸ“Š RÃ©sultats

### âœ”ï¸ EntraÃ®nement Colab (1 Ã©poque, 2000 train / 500 eval)
- Perte validation : **0.478**  
- Perte entraÃ®nement : **0.703**  

### âœ”ï¸ EntraÃ®nement Jupyter (3 Ã©poques, 114 steps au total)
- Perte validation : **0.009**  
- Perte entraÃ®nement : **0.173**  
- PrÃ©cision sur exemples personnalisÃ©s : **100%**  

---

## ğŸ’¬ Exemples de prÃ©dictions

EntrÃ©e :  
`"Quantum computing makes another leap forward."`  
â†’ **Classe prÃ©dite : tech**  

EntrÃ©e :  
`"The championship game ended in a thrilling overtime."`  
â†’ **Classe prÃ©dite : sports**  

EntrÃ©e :  
`"Researchers discover a new species of dinosaur."`  
â†’ **Classe prÃ©dite : science**  

---

## ğŸ’¾ Arborescence du projet

```
lyra_transformer/                       
â”œâ”€â”€ README.md                          # PrÃ©sentation et instructions du projet - version en anglais
â”œâ”€â”€ README_fr.md                       # PrÃ©sentation et instructions du projet - version en franÃ§ais (ce fichier)
â”œâ”€â”€ code/                              # Scripts dâ€™entraÃ®nement et dâ€™infÃ©rence
â”‚   â”œâ”€â”€ transformer_Google_collab.ipynb  # Notebook Colab pour prototypage rapide
â”‚   â”œâ”€â”€ transformer_Google_collab.py      # Script Python Ã©quivalent au workflow Colab
â”‚   â””â”€â”€ transformer_jupyter_notebook.md   # RÃ©sumÃ© markdown compatible Jupyter
â””â”€â”€ datasets/                          # Jeux de donnÃ©es CSV locaux pour le fine-tuning
    â”œâ”€â”€ train.csv                     # Jeu dâ€™entraÃ®nement (exemples Ã©tiquetÃ©s)
    â””â”€â”€ validation.csv                # Jeu de validation (pour Ã©valuation)
```

---

## ğŸš€ Utilisation

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("MODEL_TRANSFORMER")
model = AutoModelForSequenceClassification.from_pretrained("MODEL_TRANSFORMER")

text = "Apple launches its next-gen AI chip."
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)

with torch.no_grad():
    logits = model(**inputs).logits
    pred = torch.argmax(logits, dim=1).item()

label_mapping = {0: "business", 1: "science", 2: "sports", 3: "tech"}
print(label_mapping[pred])
```

---

## ğŸ§­ Notes

- Ce projet fait partie du corpus **Lyra â€” IA appliquÃ©e Ã  la formation et Ã  lâ€™automatisation Ã©cologique**.  
- ConÃ§u pour lâ€™interprÃ©tabilitÃ©, la reproductibilitÃ© et une prÃ©paration au contexte post-AGI.  

---

ğŸ§¬ *ConÃ§u par JÃ©rÃ´me â€” ingÃ©nierie IA prÃªte pour lâ€™Ã¨re post-AGI.*
